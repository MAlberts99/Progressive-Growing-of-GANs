{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GAN Trainer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPnxVijqNaCIyPqzkDyduzB"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"MsTCmKDtzmry","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605990933973,"user_tz":0,"elapsed":2292,"user":{"displayName":"Marvin Alberts","photoUrl":"","userId":"07281461137485771510"}},"outputId":"313797bc-1a2c-48d4-9ec4-6d6d496ca274"},"source":["import numpy as np\n","import torch\n","import timeit\n","import os\n","import math\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.autograd as autograd\n","\n","\n","from tqdm.notebook import tqdm\n","from google.colab import drive\n","import sys\n","\n","drive.mount(\"/content/gdrive\")\n","path = \"/content/gdrive/My Drive/Colab Notebooks/GANS/Gan V4: Progressive Growing\"\n","\n","sys.path.append(path)\n","\n","from src.Discriminator import Discriminator\n","from scr.Generator import Generator"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4qtqDlBDEDEy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605990937057,"user_tz":0,"elapsed":875,"user":{"displayName":"Marvin Alberts","photoUrl":"","userId":"07281461137485771510"}},"outputId":"2548749c-9d3b-4112-f3fa-6e0c92f85f26"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sat Nov 21 20:35:36 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q143y1wAfdJy"},"source":["class Logger():\n","  def __init__(self, run_path):\n","    self.writer = SummaryWriter(log_dir=os.path.join(run_path, \"TensorBoard\"))\n","  \n","  def add_loss(self, disc_loss, gen_loss, step):\n","    self.writer.add_scalar(\"Discriminator Loss\", disc_loss, global_step= step)\n","    self.writer.add_scalar(\"Generator Loss\", gen_loss, global_step= step)\n","  \n","  def add_images(self, name, image):\n","    self.writer.add_image(name, image)\n","  \n","  def close(self):\n","    self.writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LfFLhDHj_TD"},"source":["def calc_gradient_penalty(netD, real_data, fake_data):\n","    #print real_data.size()\n","    batch_size = real_data.shape[0]\n","    alpha = torch.rand(batch_size, 1, 1, 1)\n","    alpha = alpha.expand(real_data.shape).to(device)\n","\n","    interpolates = (alpha * real_data + ((1 - alpha) * fake_data))\n","    interpolates.requires_grad = True\n","\n","    disc_interpolates = netD(interpolates)\n","\n","    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n","                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n","                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n","\n","    gradient_penalty = torch.mean(((gradients.norm(2, dim=1) - 1) ** 2), dim=[0, 1, 2])\n","    return gradient_penalty"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"23_9orPW0q6n"},"source":["class ProgTrainer():\n","  def __init__(self, \n","               netG, # Generator\n","               netD, # Discriminator\n","               dataset_path = None, # Dataset Path\n","               dataset_class = None, # The class to be used as datasetclass\n","               out_res = 1024, # Out resolution\n","               config=None, # Configuration listing: out_res, intervals, training_schedule etc\n","               load_checkpoint_path = None,\n","               device = \"cpu\" # Cuda or cpu\n","               ):\n","\n","    # Objects\n","    self.netG = netG\n","    self.netD = netD\n","    self.optimD = None # Either make new or load from state\n","    self.optimG = None\n","    self.dataset = None\n","    self.dataset_path = dataset_path\n","    self.dataset_class = dataset_class\n","    self.data_loader = None # Either make new or load from state\n","    self.config = config\n","    self.out_res = out_res\n","    self.device = device\n","\n","    # Loss Variables\n","    self.Lambda = 10.0\n","    #self.gradient_penalty = WGANGPGradientPenalty()\n","\n","    # Default Config\n","    if config is None: # Config determines batch size and amount of images shown during growing and stabilising\n","      self.config = {\"interval\": 800000, \"4\":16, \"8\":16, \"16\":16, \"32\":16, \"64\":16, \"128\":16, \"256\":14, \"512\":6, \"1024\":3}\n","\n","    # State Variables\n","    self.schedule = self.make_schedule() # Schedule contains how many steps are required for the training of each layer\n","    self.cur_layer = 0 # Which layer is currently being trained e.g 0 is 4x4 stab, 1 is 8x8 growing, 2 is 8x8 stab etc.\n","    self.cur_step = 0 # Tracks the steps in the training of one layer\n","    self.growing = False # Wether the network is currently growing\n","    self.current_batch_size = self.config[\"4\"] # Initial Batch size \n","    self.current_res = 4\n","\n","    # Fixed noise for monitoring training progress\n","    self.fixed_noise = self.get_noise(16, 512)\n","\n","    # Load Checkpoint to resume training\n","    if load_checkpoint_path is not None and os.path.isfile(load_checkpoint_path): # Loads from Checkpoint\n","      self.load_checkpoint(load_checkpoint_path)\n","    \n","    # Sanity check that both dataset path and class exist\n","    assert self.dataset_class is not None and self.dataset_path is not None\n","    # Make dataset -> loader and optim\n","    self.update_loader_optim()\n","\n","    if load_checkpoint_path is not None and os.path.isfile(load_checkpoint_path):\n","      self.load_optim(load_checkpoint_path)\n","\n","    # Backward Tensors\n","    self.one = torch.ones(1).to(self.device)\n","    self.mone = -self.one\n","\n","\n","  #### Util Functions ####\n","  def make_schedule(self):\n","    training_phases = int((math.log2(self.out_res)-1)*2) # How many training phases there are, e.g 3 for 8x8 output i.e. 2x growing 1x stabilising\n","    \n","    schedule = [self.config[\"interval\"]//self.config[str(2**((int(i)//2)+2))]  for i in range(1,training_phases)] # How many steps are required for each layer\n","\n","    return schedule\n","\n","  def update_loader_optim(self):\n","    # Resize images to appropriate size\n","    transform = transforms.Compose([transforms.Resize(size=[self.current_res, self.current_res]),\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize(mean=0, std=1)\n","                                    ])\n","    \n","    self.dataset = self.dataset_class(root=self.dataset_path, transform=transform)\n","    \n","    # Make dataloader with appropriate batch size, use of iter to allow iteration over non epoch intervals\n","    self.data_loader = iter(DataLoader(self.dataset, batch_size=self.current_batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True))\n","\n","    if self.optimD is None or self.optimG is None: # If first training pass, no growing from first block\n","      self.optimD = optim.Adam(self.netD.parameters(), lr=0.001, betas=(0, 0.99))\n","      self.optimG = optim.Adam(self.netG.parameters(), lr=0.001, betas=(0, 0.99))\n","\n","    else:\n","      self.netD.add_block()\n","      self.netG.add_block()\n","\n","      # Move newly added blocks to device\n","      self.netD.to(self.device)\n","      self.netG.to(self.device)\n","\n","      # Make new optimiser if new Block is added, ignore previous state\n","      self.optimD = optim.Adam(self.netD.parameters(), lr=0.001, betas=(0, 0.99))\n","      self.optimG = optim.Adam(self.netG.parameters(), lr=0.001, betas=(0, 0.99))\n","\n","  def maintenance(self):\n","    if self.cur_layer >= len(self.schedule): # Sanity check to see if loop should continue or not\n","      return\n","\n","    ## Sets growing ##\n","    self.growing = not self.growing\n","\n","    ## Updates Batch Size ##\n","    state = (self.cur_layer+1)//2 # 0 for 4x4 stab + growing, 1 for 8x8 stab + growing, etc\n","    self.current_res = int(2**(state+2)) # calculate current resolution from state\n","    self.current_batch_size = self.config[str(self.current_res)] # get batch_size from config\n","\n","    ## Update Loader, Optim, Net ##\n","    if self.growing:\n","      print(\"Updating Loader, optim\")\n","      self.update_loader_optim()\n","\n","  # Get alpha, simple division\n","  def get_alpha(self):\n","    if not self.growing:\n","      return 1.0\n","    \n","    alpha = self.cur_step/self.schedule[self.cur_layer]\n","    return alpha\n"," \n","  def load_checkpoint(self, checkpoint_path):\n","    state_dict = torch.load(checkpoint_path)\n","\n","    # Add as many blocks as required\n","    for i in range((state_dict[\"cur_layer\"]+1)//2):\n","      self.netG.add_block()\n","      self.netD.add_block()\n","\n","    # Load net and opt state dicts\n","    self.netG.load_state_dict(state_dict[\"Generator\"])\n","    self.netG.to(self.device)\n","    self.netD.load_state_dict(state_dict[\"Discriminator\"])\n","    self.netD.to(self.device)\n","\n","    # Set other variables\n","    self.dataset_path = state_dict[\"DPath\"]\n","    self.dataset_class = state_dict[\"DClass\"]\n","    self.config = state_dict[\"config\"]\n","    self.out_res = state_dict[\"out_res\"]\n","    self.schedule = state_dict[\"schedule\"]\n","    self.cur_layer = state_dict[\"cur_layer\"]\n","    self.cur_step = state_dict[\"cur_step\"]\n","    self.growing = state_dict[\"growing\"]\n","    self.current_batch_size = state_dict[\"current_batch_size\"]\n","    self.current_res = state_dict[\"current_res\"]\n","\n","    self.fixed_noise = state_dict[\"fixed noise\"]\n","\n","  # Load Optimiser later as they're created by the class\n","  def load_optim(self, checkpoint_path):\n","    state_dict = torch.load(checkpoint_path)\n","    self.optimG.load_state_dict(state_dict[\"Optim G\"])\n","    self.optimD.load_state_dict(state_dict[\"Optim D\"])\n","\n","  def save_checkpoint(self, path):\n","    name = os.path.join(path, \"Layer: {} Step: {}.tar\".format(self.cur_layer, self.cur_step))\n","    state_dict = { # Fixed Variables and Objects\n","                  \"Generator\" : self.netG.state_dict(),\n","                  \"Discriminator\" : self.netD.state_dict(),\n","                  \"Optim D\" : self.optimD.state_dict(),\n","                  \"Optim G\" : self.optimG.state_dict(),\n","                  \"DPath\" : self.dataset_path,\n","                  \"DClass\" : self.dataset_class,\n","                  \"config\" : self.config,\n","                  \"out_res\" : self.out_res,\n","\n","                  # State Variables\n","                  \"schedule\" : self.schedule,\n","                  \"cur_layer\" : self.cur_layer,\n","                  \"cur_step\" : self.cur_step ,\n","                  \"growing\" : self.growing,\n","                  \"current_batch_size\" : self.current_batch_size,\n","                  \"current_res\" : self.current_res,\n","\n","                  \"fixed noise\" : self.fixed_noise\n","                  }\n","    torch.save(state_dict, name)\n","    print(\"Saved {} succesfully\".format(\"Layer: {} Step: {}.tar\".format(self.cur_layer, self.cur_step)))\n","\n","\n","  #### Train and Step ####\n","  def train(self, run_path):\n","    logger = Logger(run_path)\n","\n","    # Use of while loops to allow easy resuming, for loops would work as well but would not allow resuming from a given point\n","    while self.cur_layer < len(self.schedule): \n","      with tqdm(total=self.schedule[self.cur_layer]) as pbar:\n","        while self.cur_step < self.schedule[self.cur_layer]:\n","        \n","          try:\n","           img = next(self.data_loader).to(device) # \n","          except:\n","            self.data_loader = iter(DataLoader(self.dataset, batch_size=self.current_batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True))\n","            img = next(self.data_loader)[0].to(device)\n","\n","          disc_loss, gen_loss = self.step(img, growing = self.growing, alpha=self.get_alpha())\n","          if self.cur_step%4 == 0:\n","            pbar.set_postfix_str(\"Layer: {}, Step: {}, Res: {}, Alpha: {:.3f}, Growing: {}, Disc_Loss: {:.4f}, Gen_Loss: {:.4f}\".format(self.cur_layer,\n","                                                                                 self.cur_step,\n","                                                                                 self.current_res,\n","                                                                                 self.get_alpha(),\n","                                                                                 self.growing,\n","                                                                                 disc_loss, gen_loss))\n","          \n","          self.cur_step +=1\n","          pbar.n = self.cur_step # Allows pbar to be set to appropiate position if resuming from save\n","          pbar.refresh() # If resuming messes with ETA though\n","\n","          # Logging\n","          if self.cur_step% (self.schedule[self.cur_layer]//100) == 0:\n","            time_step = sum(self.schedule[0:self.cur_layer]) + self.cur_step\n","            logger.add_loss(disc_loss, gen_loss, time_step)\n","\n","          if self.cur_step% (self.schedule[self.cur_layer]//30) == 0: # Takes snapshot 40 times per layer\n","            fake_img = netG(self.fixed_noise).detach().cpu()\n","            fake_img = torchvision.utils.make_grid(fake_img.clamp(min=-1, max=1), nrow=4, scale_each=True, normalize=True)\n","\n","            name = sum(self.schedule[0:self.cur_layer]) + self.cur_step\n","            logger.add_images(str(name), fake_img)\n","          \n","          if self.cur_step % (self.schedule[self.cur_layer]//2) == 0: # Saves 2 times per layer\n","            logger.close() # Save log file to disk\n","            logger = Logger(run_path) # Make new Logger\n","            self.save_checkpoint(run_path) # Save Checkpoint\n","\n","          \n","      self.cur_step = 0\n","      self.cur_layer += 1\n","\n","      self.maintenance()\n","\n","    print(\"Finished\")\n","  \n","  # Get noise for generator from Normal (0, 1)\n","  def get_noise(self, batch_size, latent_size):\n","    return torch.randn(batch_size, latent_size).to(self.device)\n","\n","  # Generator and Discriminator step\n","  def step(self, img, growing=False, alpha=1.0):\n","    batch_size = img.shape[0]\n","\n","    # Discriminator Step \n","    for p in netD.parameters():\n","        p.requires_grad = True\n","\n","    real_logits = self.netD(img, growing = growing, alpha= alpha)\n","    real_loss = torch.mean(real_logits, 0)\n","\n","    real_loss = real_loss - 1e-4 * torch.square(real_loss) # - Correction term \n","    real_loss.backward(self.mone) # Multiply with -1 => gets correct sign\n","\n","    with torch.no_grad():\n","      noise = self.get_noise(batch_size, 512)\n","      fake_img = self.netG(noise, growing = growing, alpha = alpha)\n","\n","    fake_logits = self.netD(fake_img, growing = growing, alpha = alpha)\n","    fake_loss = torch.mean(fake_logits, 0)\n","    fake_loss.backward(self.one) # Add to previous loss\n","\n","    gradient_penalty = calc_gradient_penalty(netD, img.detach(), fake_img.detach()) * 10\n","    gradient_penalty.backward() # Add gradient penalty\n","\n","    Wasserstein_Loss = real_loss - fake_loss   \n","\n","    # Update weights\n","    self.optimD.step()\n","\n","    # Generator Step\n","    for p in netD.parameters():\n","        p.requires_grad = False  # to avoid computation\n","    \n","    netG.zero_grad()\n","\n","    noise = self.get_noise(batch_size, 512)\n","    fake_img = self.netG(noise, growing = growing)\n","    fake_logits = self.netD(fake_img, growing = growing)\n","    fake_loss = torch.mean(fake_logits, 0)\n","    fake_loss.backward(self.mone)\n","    gen_loss = -fake_loss\n","    \n","    self.optimG.step()\n","\n","    return Wasserstein_Loss.item(), gen_loss.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrDFtbyOLJAx"},"source":["root = \"FashionMNIST\"\n","transform = transforms.Compose([transforms.ToTensor()])\n","dset = torchvision.datasets.FashionMNIST(root, transform=transform, download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-MrAluGMTMJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605990992449,"user_tz":0,"elapsed":1417,"user":{"displayName":"Marvin Alberts","photoUrl":"","userId":"07281461137485771510"}},"outputId":"41bce6cb-2268-4df9-81f6-52bf7cf9337d"},"source":["device = \"cuda\"\n","\n","netG = Generator(out_dim=1, out_res=32).to(device)\n","netD = Discriminator(in_dim=1).to(device)\n","config = {\"interval\":150000, \"4\":16, \"8\":16, \"16\":16, \"32\":16}\n","\n","run_path = os.path.join(os.path.join(path, \"Run 1\"))\n","\n","pTrainer = ProgTrainer(netG, netD, dataset_path=root, dataset_class=torchvision.datasets.FashionMNIST,\n","                       device = device, config=config, out_res=32, )#load_checkpoint_path = os.path.join(run_path, \"Layer: 1 Step: 5000.tar\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["512 512\n","512 512\n","512 512\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VqGGOln4OuKb"},"source":["pTrainer.train(run_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uL8KSCHSlt0N","colab":{"base_uri":"https://localhost:8080/","height":59},"executionInfo":{"elapsed":482,"status":"ok","timestamp":1602705916771,"user":{"displayName":"Marvin Alberts","photoUrl":"","userId":"07281461137485771510"},"user_tz":-60},"outputId":"c3db8e90-d86d-4b40-a5d0-705f46a3d278"},"source":["toPIL = transforms.ToPILImage(mode=\"RGB\")\n","\n","noise = pTrainer.get_noise(16, 512)\n","img = netG(noise).cpu()\n","img = torchvision.utils.make_grid(img.clamp(min=-1, max=1), nrow=4, scale_each=True, normalize=True)\n","img = toPIL(img)\n","img"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACoAAAAqCAIAAABKoV4MAAABxklEQVR4nO2Wu6rqQBhG/ySHaBgSHDUk8RJ0xAStJIKFVSpfwULwLXwDK9/BdxEEsbOys0hjEbyAl0SLSdxFNla72XCOCmdWM8UqVv7mIwD/M1zyxHH8o+Z5/p9a/ndf+7dheZZ/L6PRCAAQQrqua5omy3KtVsMYJ9YwDEmSMMaKogBAPp9fr9eu6ya23+83Gg0A4DiOEFIulweDAUIosc1ms16vG4ZRLBY5jgMA27aHw+HLT/xA2OqxPMu/gT/J0+12l8tlNpullF4uFwDQdf1+vyeWEOJ5nizLYRhKkiSKYqlUOp1Oie31evP5nFIaRRHHcaIoptPpQqGQ2Ha7vVqtDMPAGMdxLAiCZVm+77/80g+ErR7Ls/wb+F69arWqaRohZDabUUr3+z0h5LlNnU4nDEOEUBAEnueFYUgISaVSia1UKsfj8Xq9Kooiy7LrutPp1LbtxKqqSikNgoBSapomAAiCIAjCyy/9QNjqsTzLv4Hv1XNdd7FYUEoxxqqqRlGEEHr+zVmWtdlsAEDTtMfjwfP8ZDIZj8eJbbVa5/PZ9/3b7eY4zuFwyGQyz4DjOEEQIIS22y0A7HY70zRzudwrj/xUvgAsD74x8SOI/QAAAABJRU5ErkJggg==\n","text/plain":["<PIL.Image.Image image mode=RGB size=42x42 at 0x7F22763E01D0>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"YyUSEohqCWOa"},"source":[""],"execution_count":null,"outputs":[]}]}